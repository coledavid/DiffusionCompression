{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:40:03.360312900Z",
     "start_time": "2024-04-25T17:40:03.349828100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -U -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "#!echo '{add your relevant info here}' > ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d heyitsfahd/nature\n",
    "!mkdir nature_kaggle\n",
    "!unzip nature.zip -d nature_kaggle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2787ce4fd759d1de",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adapted From https://medium.com/@adityanutakki6250/sr3-explained-and-implemented-in-pytorch-from-scratch-b43b9742c232\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels = 3, output_channels = 3, time_steps = 512):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.time_steps = time_steps\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        self.e1 = encoder_block(self.input_channels, 64, time_steps=self.time_steps)\n",
    "        self.e2 = encoder_block(64, 128, time_steps=self.time_steps)\n",
    "        # self.da2 = AttnBlock(128)\n",
    "        self.e3 = encoder_block(128, 256, time_steps=self.time_steps)\n",
    "        self.da3 = AttnBlock(256)\n",
    "        self.e4 = encoder_block(256, 512, time_steps=self.time_steps)\n",
    "        self.da4 = AttnBlock(512)\n",
    "\n",
    "        self.b = conv_block(512, 1024, time_steps=self.time_steps) # bottleneck\n",
    "        self.ba1 = AttnBlock(1024)\n",
    "        self.d1 = decoder_block(1024, 512, time_steps=self.time_steps)\n",
    "        self.ua1 = AttnBlock(512)\n",
    "        self.d2 = decoder_block(512, 256, time_steps=self.time_steps)\n",
    "        self.ua2 = AttnBlock(256)\n",
    "        self.d3 = decoder_block(256, 128, time_steps=self.time_steps)\n",
    "        # self.ua3 = AttnBlock(128)\n",
    "        self.d4 = decoder_block(128, 64, time_steps=self.time_steps)\n",
    "        # self.ua4 = AttnBlock(64)\n",
    "        self.outputs = nn.Conv2d(64, self.output_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs, t = None):\n",
    "        # downsampling block\n",
    "        s1, p1 = self.e1(inputs, t)\n",
    "        s2, p2 = self.e2(p1, t)\n",
    "        s3, p3 = self.e3(p2, t)\n",
    "        p3 = self.da3(p3)\n",
    "        s4, p4 = self.e4(p3, t)\n",
    "        p4 = self.da4(p4)\n",
    "        # bottleneck\n",
    "        b = self.b(p4, t)\n",
    "        b = self.ba1(b)\n",
    "        # upsampling block\n",
    "        d1 = self.d1(b, s4, t)\n",
    "        d1 = self.ua1(d1)\n",
    "        d2 = self.d2(d1, s3, t)\n",
    "        d2 = self.ua2(d2)\n",
    "        d3 = self.d3(d2, s2, t)\n",
    "        d4 = self.d4(d3, s1, t)\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:40:05.445088Z",
     "start_time": "2024-04-25T17:40:05.436111900Z"
    }
   },
   "id": "3b2890fa65526047",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adapted From https://medium.com/@adityanutakki6250/sr3-explained-and-implemented-in-pytorch-from-scratch-b43b9742c232\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, embedding_dims, num_heads = 4) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.ln = nn.LayerNorm(embedding_dims)\n",
    "        self.mhsa = MultiHeadSelfAttention(embedding_dims = embedding_dims, num_heads = num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.LayerNorm(self.embedding_dims),\n",
    "            nn.Linear(self.embedding_dims, self.embedding_dims),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.embedding_dims, self.embedding_dims),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        bs, c, sz, _ = x.shape\n",
    "        x = x.view(-1, self.embedding_dims, sz * sz).swapaxes(1, 2) # is of the shape (bs, sz**2, self.embedding_dims)\n",
    "        x_ln = self.ln(x)\n",
    "        _, attention_value = self.mhsa(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, c, sz, sz)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dims, num_heads = 4) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.num_heads = num_heads\n",
    "        assert self.embedding_dims % self.num_heads == 0, f\"{self.embedding_dims} not divisible by {self.num_heads}\"\n",
    "        self.head_dim = self.embedding_dims // self.num_heads\n",
    "        self.wq = nn.Linear(self.head_dim, self.head_dim)\n",
    "        self.wk = nn.Linear(self.head_dim, self.head_dim)\n",
    "        self.wv = nn.Linear(self.head_dim, self.head_dim)\n",
    "        self.wo = nn.Linear(self.embedding_dims, self.embedding_dims)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        # no need for a mask\n",
    "        attn_weights = F.softmax((q @ k.transpose(-1, -2))/self.head_dim**0.5, dim = -1)\n",
    "        return attn_weights, attn_weights @ v\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        bs, img_sz, c = q.shape\n",
    "        q = q.view(bs, img_sz, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(bs, img_sz, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(bs, img_sz, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # q, k, v of the shape (bs, self.num_heads, img_sz**2, self.head_dim)\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        attn_weights, o = self.attention(q, k, v) # of shape (bs, num_heads, img_sz**2, c)\n",
    "\n",
    "        o = o.transpose(1, 2).contiguous().view(bs, img_sz, self.embedding_dims)\n",
    "        o = self.wo(o)\n",
    "        return attn_weights, o"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:40:07.421122Z",
     "start_time": "2024-04-25T17:40:07.412146Z"
    }
   },
   "id": "14603bd744dbc5ee",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adapted From https://medium.com/@adityanutakki6250/sr3-explained-and-implemented-in-pytorch-from-scratch-b43b9742c232\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_steps, activation = \"relu\"):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c, time_steps = time_steps, activation = activation, embedding_dims = out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs, time = None):\n",
    "        x = self.conv(inputs, time)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "# Decoder Block for upsampling\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_steps, activation = \"relu\"):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c, time_steps = time_steps, activation = activation, embedding_dims = out_c)\n",
    "    def forward(self, inputs, skip, time = None):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x, time)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:40:07.445058200Z",
     "start_time": "2024-04-25T17:40:07.423116700Z"
    }
   },
   "id": "65bad7f297bb590a",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adapted From https://medium.com/@adityanutakki6250/sr3-explained-and-implemented-in-pytorch-from-scratch-b43b9742c232\n",
    "class GammaEncoding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.act = nn.LeakyReLU()\n",
    "    def forward(self, noise_level):\n",
    "        count = self.dim // 2\n",
    "        step = torch.arange(count, dtype=noise_level.dtype, device=noise_level.device) / count\n",
    "        encoding = noise_level.unsqueeze(1) * torch.exp(log(1e4) * step.unsqueeze(0))\n",
    "        encoding = torch.cat([torch.sin(encoding), torch.cos(encoding)], dim=-1)\n",
    "        return self.act(self.linear(encoding))\n",
    "\n",
    "# Double Conv Block\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_steps = 1000, activation = \"relu\", embedding_dims = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.embedding_dims = embedding_dims if embedding_dims else out_c\n",
    "\n",
    "        # self.embedding = nn.Embedding(time_steps, embedding_dim = self.embedding_dims)\n",
    "        self.embedding = GammaEncoding(self.embedding_dims)\n",
    "        # switch to nn.Embedding if you want to pass in timestep instead; but note that it should be of dtype torch.long\n",
    "        self.act = nn.ReLU() if activation == \"relu\" else nn.SiLU()\n",
    "\n",
    "    def forward(self, inputs, time = None):\n",
    "        time_embedding = self.embedding(time).view(-1, self.embedding_dims, 1, 1)\n",
    "        # print(f\"time embed shape => {time_embedding.shape}\")\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = x + time_embedding\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:40:09.376273500Z",
     "start_time": "2024-04-25T17:40:09.367782100Z"
    }
   },
   "id": "af6fd661b62e6d82",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adapted From https://medium.com/@adityanutakki6250/sr3-explained-and-implemented-in-pytorch-from-scratch-b43b9742c232\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, time_steps,\n",
    "                 beta_start = 10e-4,\n",
    "                 beta_end = 0.02,\n",
    "                 image_dims = (3, 128, 128)):\n",
    "\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.image_dims = image_dims\n",
    "        c, h, w = self.image_dims\n",
    "        self.img_size, self.input_channels = h, c\n",
    "        self.betas = torch.linspace(beta_start, beta_end, self.time_steps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_hats = torch.cumprod(self.alphas, dim = -1)\n",
    "        self.model = UNet(input_channels = 2*c, output_channels = c, time_steps = self.time_steps)\n",
    "\n",
    "    def add_noise(self, x, ts):\n",
    "        # 'x' and 'ts' are expected to be batched\n",
    "        noise = torch.randn_like(x)\n",
    "        # print(x.shape, noise.shape)\n",
    "        noised_examples = []\n",
    "        for i, t in enumerate(ts):\n",
    "            alpha_hat_t = self.alpha_hats[t]\n",
    "            noised_examples.append(torch.sqrt(alpha_hat_t)*x[i] + torch.sqrt(1 - alpha_hat_t)*noise[i])\n",
    "        return torch.stack(noised_examples), noise\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.model(x, t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:40:09.392229800Z",
     "start_time": "2024-04-25T17:40:09.378268700Z"
    }
   },
   "id": "aca85ff18cba4b09",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms import transforms\n",
    "import os, cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Adapted From https://medium.com/@adityanutakki6250/sr3-explained-and-implemented-in-pytorch-from-scratch-b43b9742c232\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, dataset_path, limit = -1, _transforms = None, hr_sz = 128, lr_sz = 32) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transforms = _transforms\n",
    "\n",
    "        if not self.transforms:\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomHorizontalFlip(p = 0.5),\n",
    "                transforms.ColorJitter([0.5, 1]),\n",
    "                transforms.RandomAdjustSharpness(1.1, p = 0.4),\n",
    "                transforms.Normalize((0.5, ), (0.5,)) # normalizing image with mean, std = 0.5, 0.5\n",
    "            ])\n",
    "        self.hr_sz, self.lr_sz = transforms.Resize((hr_sz, hr_sz), interpolation=InterpolationMode.BICUBIC), transforms.Resize((lr_sz, lr_sz), interpolation=InterpolationMode.BICUBIC)\n",
    "\n",
    "        self.dataset_path, self.limit = dataset_path, limit\n",
    "        self.valid_extensions = [\"jpg\", \"jpeg\", \"png\", \"JPEG\", \"JPG\"]\n",
    "\n",
    "        self.images_path = dataset_path\n",
    "        self.images = os.listdir(self.images_path)[:self.limit]\n",
    "        self.images = [os.path.join(self.images_path, image) for image in self.images if image.split(\".\")[-1] in self.valid_extensions]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.images[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image)\n",
    "        hr_image, lr_image = self.hr_sz(image), self.lr_sz(image)\n",
    "        # the core idea here is resizing the (128, 128) down to a lower resolution and then back up to (128, 128)\n",
    "        return self.hr_sz(lr_image), hr_image # the hr_image is 'y' and low res image scaled to (128, 128) is our 'x'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:40:11.575269Z",
     "start_time": "2024-04-25T17:40:11.482515100Z"
    }
   },
   "id": "35b1a1f03ae3f407",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from time import time\n",
    "# Adapted From https://medium.com/@adityanutakki6250/sr3-explained-and-implemented-in-pytorch-from-scratch-b43b9742c232\n",
    "def train_ddpm(time_steps = 2000, epochs = 20, batch_size = 16, device = \"cuda:0\", image_dims = (3, 128, 128), low_res_dims = (3, 32, 32)):\n",
    "    ddpm = DiffusionModel(time_steps = time_steps)\n",
    "    c, hr_sz, _ = image_dims\n",
    "    _, lr_sz, _ = low_res_dims\n",
    "\n",
    "    ds = SRDataset(\"C:/Users/David/DataspellProjects/Pytorch_Deep_Learning/Diffusion_Project/archive/Nature/x128\", hr_sz = hr_sz, lr_sz = lr_sz)\n",
    "    print(ds.__len__())\n",
    "    loader = DataLoader(ds, batch_size = batch_size, shuffle = True, drop_last = True, num_workers = 2)\n",
    "    opt = torch.optim.Adam(ddpm.model.parameters(), lr = 1e-3)\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    ddpm.model.to(device)\n",
    "    print()\n",
    "    for ep in range(epochs):\n",
    "        ddpm.model.train()\n",
    "        print(f\"Epoch {ep}:\")\n",
    "        losses = []\n",
    "        stime = time()\n",
    "\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "\n",
    "            # 'y' represents the high-resolution target image, while 'x' represents the low-resolution image to be conditioned upon.\n",
    "\n",
    "            bs = y.shape[0]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            ts = torch.randint(low = 1, high = ddpm.time_steps, size = (bs, ))\n",
    "            gamma = ddpm.alpha_hats[ts].to(device)\n",
    "            ts = ts.to(device = device)\n",
    "\n",
    "            y, target_noise = ddpm.add_noise(y, ts)\n",
    "            y = torch.cat([x, y], dim = 1)\n",
    "\n",
    "            predicted_noise = ddpm.model(y, gamma)\n",
    "            loss = criterion(target_noise, predicted_noise)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if i % 250 == 0:\n",
    "                print(f\"Loss: {loss.item()}; step {i}; epoch {ep}\")\n",
    "\n",
    "        ftime = time()\n",
    "        print(f\"Epoch trained in {ftime - stime}s; Avg loss => {sum(losses)/len(losses)}\")\n",
    "\n",
    "        torch.save(ddpm.state_dict(), f\"./sr_ep_{ep}.pt\")\n",
    "        print(\"The above training loop saves a model at the end of every epoch and prints the loss after every 250 steps.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:40:14.966148800Z",
     "start_time": "2024-04-25T17:40:14.937226700Z"
    }
   },
   "id": "494381450ce74bf9",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ddpm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_ddpm\u001B[49m(time_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2000\u001B[39m, epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16\u001B[39m, device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, image_dims \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m), low_res_dims \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m32\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_ddpm' is not defined"
     ]
    }
   ],
   "source": [
    "train_ddpm(time_steps = 2000, epochs = 30, batch_size = 16, device = \"cuda\", image_dims = (3, 128, 128), low_res_dims = (3, 32, 32))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T23:12:26.562626400Z",
     "start_time": "2024-04-07T23:12:25.844184400Z"
    }
   },
   "id": "9f389fad817b182b",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torchshow\n",
    "import time\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def sample(model, lr_img, device = \"cuda\", im_name = \"sample.jpeg\"):\n",
    "\n",
    "    resize = transforms.Resize((128, 128), interpolation=InterpolationMode.BICUBIC)\n",
    "    lr_img = resize(lr_img)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    #stime = time()\n",
    "    with torch.no_grad():\n",
    "        y = torch.randn_like(lr_img, device = device)\n",
    "        lr_img = lr_img.to(device)\n",
    "        for i, t in enumerate(range(model.time_steps - 1, 0 , -1)):\n",
    "            alpha_t, alpha_t_hat, beta_t = model.alphas[t], model.alpha_hats[t], model.betas[t]\n",
    "\n",
    "            t = torch.tensor(t, device = device).long()\n",
    "            pred_noise = model(torch.cat([lr_img, y], dim = 1), alpha_t_hat.view(-1).to(device))\n",
    "            y = (torch.sqrt(1/alpha_t))*(y - (1-alpha_t)/torch.sqrt(1 - alpha_t_hat) * pred_noise)\n",
    "            if t > 1:\n",
    "                noise = torch.randn_like(y)\n",
    "                y = y + torch.sqrt(beta_t) * noise\n",
    "\n",
    "\n",
    "    save_image(y, im_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:12:11.062553600Z",
     "start_time": "2024-04-25T23:12:10.884001300Z"
    }
   },
   "id": "1213b79c1edc2892",
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "def load_image(image_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to a tensor\n",
    "    img_tensor = ToTensor()(img)\n",
    "\n",
    "    # Add an extra dimension for batch size\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    return img_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T02:51:53.983989500Z",
     "start_time": "2024-04-15T02:51:53.905162800Z"
    }
   },
   "id": "17140ddb600c4724",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "\n",
    "def load_images_from_folder(folder_path):\n",
    "    image_files = os.listdir(folder_path)\n",
    "    image_files.sort()  # Sort the filenames\n",
    "    image_tensors = []\n",
    "\n",
    "    for image_file in image_files:\n",
    "        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            img = Image.open(image_path)\n",
    "            img_tensor = ToTensor()(img)\n",
    "            img_tensor = img_tensor.unsqueeze(0)\n",
    "            image_tensors.append(img_tensor)\n",
    "\n",
    "    image_batch = torch.cat(image_tensors, dim=0)\n",
    "    return image_batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:34:35.145951200Z",
     "start_time": "2024-04-25T23:34:35.122987300Z"
    }
   },
   "id": "d46d078aadb6357a",
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "image_batch = load_images_from_folder('DiffInputSmall')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T23:34:37.595635400Z",
     "start_time": "2024-04-25T23:34:37.549244700Z"
    }
   },
   "id": "cf524a5bb87c9934",
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Code for batch upscaling with diffusion model\n",
    "# Specify the file path to your pre-trained model \n",
    "model_path = \"sr_ep_39.pt\"\n",
    "\n",
    "# Initialize the model\n",
    "model = DiffusionModel(time_steps=2000, image_dims=(3, 128, 128))\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "# Update the model parameters\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(image_batch.shape)\n",
    "\n",
    "for i, x in enumerate(image_batch):\n",
    "    # Assuming x is your low-resolution image\n",
    "    print(x.shape)\n",
    "    x = x.unsqueeze(0)\n",
    "    hr_img = sample(model, lr_img=x, device = \"cuda\", im_name = f\"output_image_{i}.jpeg\")\n",
    "    # Save the image with a unique name\n",
    "    #save_image(hr_img, f\"output_image_{i}.jpeg\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f9eb8a3ca232798",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom made test functions for trouble shooting model\n",
    "\n",
    "def upscale_image(lr_image_path, model_path):\n",
    "    # Load the low-resolution image\n",
    "    lr_img = load_image(lr_image_path)\n",
    "    \n",
    "    # Specify the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = DiffusionModel(time_steps=2000, image_dims=(3, 128, 128))\n",
    "\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # Update the model parameters\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Move the model and image to the device\n",
    "    model = model.to(device)\n",
    "    lr_img = lr_img.to(device)\n",
    "\n",
    "    # Denoise the image\n",
    "    hr_img = sample(model, lr_img, device)\n",
    "\n",
    "    return hr_img\n",
    "\n",
    "def load_image(image_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to a tensor\n",
    "    img_tensor = ToTensor()(img)\n",
    "\n",
    "    # Add an extra dimension for batch size\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "def sample(model, lr_img, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y = torch.randn_like(lr_img, device=device)\n",
    "        for t in range(model.time_steps - 1, -1, -1):\n",
    "            alpha_t = model.alphas[t].to(device)\n",
    "            alpha_t_hat = model.alpha_hats[t].to(device)\n",
    "            beta_t = model.betas[t].to(device)\n",
    "\n",
    "            pred_noise = model(torch.cat([lr_img, y], dim=1), alpha_t_hat.view(-1).to(device))\n",
    "            y = (torch.sqrt(1/alpha_t))*(y - (1-alpha_t)/torch.sqrt(1 - alpha_t_hat) * pred_noise)\n",
    "            if t > 1:\n",
    "                noise = torch.randn_like(y)\n",
    "                y = y + torch.sqrt(beta_t) * noise\n",
    "\n",
    "    return y\n",
    "\n",
    "def denormalize(tensor):\n",
    "    return (tensor * 0.5) + 0.5\n",
    "\n",
    "def display_image(image_tensor):\n",
    "    # Remove the batch dimension\n",
    "    img = image_tensor.squeeze(0)\n",
    "\n",
    "    # Denormalize the image\n",
    "    img = denormalize(img)\n",
    "\n",
    "    # Move the image to cpu and convert to numpy\n",
    "    img = img.cpu().numpy()\n",
    "\n",
    "    # Transpose the image to [height, width, channels]\n",
    "    img = img.transpose(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def denormalize(tensor):\n",
    "    tensor = (tensor * 0.5) + 0.5\n",
    "    tensor = torch.clamp(tensor, 0, 1)  # Clip values to the range [0, 1]\n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T17:49:04.348955500Z",
     "start_time": "2024-04-25T17:49:04.254698300Z"
    }
   },
   "id": "d52fb0ac6430e4a5",
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
